{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Meli_Challenge_2020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNlXdcyoOTUGIGyT1x04Mlz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanbeleno/meli-challenge-2020/blob/main/Meli_Challenge_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEgVHVKgSZWB"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "[Mercado Libre Data Challenge 2020](https://ml-challenge.mercadolibre.com/) requires to build a ML model to predict the next item to be bought by a user based on his/her history of searches and views.\n",
        "\n",
        "We start by installing the required libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmPve_Bretyi",
        "outputId": "bb200497-2ee7-4560-a76c-ecc03e56c84b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install ujson==4.0.1\n",
        "!pip install torch==1.7.0\n",
        "!pip install sentence-transformers==0.3.8\n",
        "!pip install fastprogress==1.0.0\n",
        "!pip install scikit-learn==0.23.2\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ujson==4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/84/e039c6ffc6603f2dfe966972d345d4f650a4ffd74b18c852ece645de12ac/ujson-4.0.1-cp36-cp36m-manylinux1_x86_64.whl (179kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 23.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 30kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 40kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 51kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 61kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 71kB 10.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 81kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 92kB 9.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 102kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 112kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 122kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 133kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 143kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 153kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 163kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 174kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 9.8MB/s \n",
            "\u001b[?25hInstalling collected packages: ujson\n",
            "Successfully installed ujson-4.0.1\n",
            "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0) (0.16.0)\n",
            "Collecting sentence-transformers==0.3.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/fd/0190080aa0af78d7cd5874e4e8e85f0bed9967dd387cf05d760832b95da9/sentence-transformers-0.3.8.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.9MB/s \n",
            "\u001b[?25hCollecting transformers<3.4.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 10.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.8) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.8) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.8) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.8) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.8) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.8) (3.2.5)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 19.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers==0.3.8) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 29.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers==0.3.8) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers==0.3.8) (20.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers==0.3.8) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers==0.3.8) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->sentence-transformers==0.3.8) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->sentence-transformers==0.3.8) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers==0.3.8) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers==0.3.8) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.4.0,>=3.1.0->sentence-transformers==0.3.8) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.4.0,>=3.1.0->sentence-transformers==0.3.8) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers==0.3.8) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers==0.3.8) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers==0.3.8) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers==0.3.8) (2.10)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.8-cp36-none-any.whl size=101996 sha256=e5239caff804b47c3eff79e77cd194750673ec1db6b347ac78e9f040d818b1fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/ec/b3/d12cc8e4daf77846db6543033d3a5642f204c0320b15945647\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=bb953249d03f8ad7959e160f9dc4cc5255a8766e7cdd77d04cfbd718bf9e5998\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.3.8 sentencepiece-0.1.94 tokenizers-0.8.1rc2 transformers-3.3.1\n",
            "Requirement already satisfied: fastprogress==1.0.0 in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastprogress==1.0.0) (1.18.5)\n",
            "Collecting scikit-learn==0.23.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2) (0.17.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.23.2) (1.4.1)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.23.2 threadpoolctl-2.1.0\n",
            "Thu Nov 19 01:31:44 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIK-1_eMWFpd"
      },
      "source": [
        "Make sure you don't get disconected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXIjuMUAWIsq",
        "outputId": "3a4752d5-8087-4dec-ffd3-d51cd1d6014a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}setInterval(ClickConnect,60000)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXMQ4quYT7MW"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We setup Google Drive to access and write data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ38pMXfPx7m",
        "outputId": "63d3290b-cbe7-4036-98e8-31abb26ab091",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Setup Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/recursos_colab/meli_challenge_2020/'\n",
        "\n",
        "RAW_TRAIN_DATASET_PATH = f'{root_path}train_dataset.jl.gz'\n",
        "TRAIN_DATASET_PATH = f'{root_path}training_dataset.jl.gz'\n",
        "OVERSAMPLED_TRAIN_DATASET_PATH = f'{root_path}oversampled_training_dataset.jl.gz'\n",
        "VALIDATION_DATASET_PATH = f'{root_path}validation_dataset.jl.gz'\n",
        "ITEM_DATA_PATH = f'{root_path}item_data.jl.gz'\n",
        "TEST_DATASET_PATH = f'{root_path}test_dataset.jl.gz'\n",
        "SAMPLE_SUBMISSION_PATH = f'{root_path}sample_submission.csv'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VTFY-zbscgF"
      },
      "source": [
        "Run the code below just once, there is no need to download the files on Google Drive more than once. The files are the item data, sample submission file, test and train dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_T5yf_FiKwO",
        "outputId": "fb88e007-9129-49b3-cf44-c75d27558039",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download the files and store them in Google Drive\n",
        "import requests\n",
        "file_urls = {\n",
        "    'test_dataset.jl.gz': 'https://meli-data-challenge.s3.amazonaws.com/2020/test_dataset.jl.gz',\n",
        "    'train_dataset.jl.gz': 'https://meli-data-challenge.s3.amazonaws.com/2020/train_dataset.jl.gz',\n",
        "    'item_data.jl.gz': 'https://meli-data-challenge.s3.amazonaws.com/2020/item_data.jl.gz',\n",
        "    'sample_submission.csv': 'https://meli-data-challenge.s3.amazonaws.com/2020/sample_submission.csv'\n",
        "}\n",
        "\n",
        "# Source: https://stackoverflow.com/q/62285313\n",
        "for file_name in file_urls:\n",
        "  r = requests.get(file_urls[file_name], stream = True)\n",
        "  with open(f'{root_path}{file_name}', \"wb\") as file:\n",
        "      for block in r.iter_content(chunk_size = 1024):\n",
        "          if block:\n",
        "              file.write(block)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9DZcJZLVzMW"
      },
      "source": [
        "## Data exploration\n",
        "\n",
        "The files have a Gzipped JSON Lines format and they use most of the RAM available on Google Colab (12GB), so I decided read the files line by line to save some RAM for model training usage. At this point, I'm only interested on two files: ```item_data.jl.gz``` and ```train_dataset.jl.gz```because they are used to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgaTjgYbZhhg"
      },
      "source": [
        "import gzip\n",
        "import ujson\n",
        "\n",
        "def print_first_lines(file_path, n=3):\n",
        "    \"\"\"Print the frist line of JSON Line file using identation of 4\"\"\"\n",
        "    with gzip.open(file_path,'rt') as f:\n",
        "        for index, line in enumerate(f):\n",
        "            data = ujson.loads(line)\n",
        "            print(ujson.dumps(data, indent=4))\n",
        "            if index >= n-1:\n",
        "              break\n",
        "\n",
        "def blocks(files, size=65536):\n",
        "    while True:\n",
        "        b = files.read(size)\n",
        "        if not b: break\n",
        "        yield b\n",
        "\n",
        "def print_num_lines(file_path):\n",
        "    \"\"\"Count the number of lines in a file\"\"\"\n",
        "    with gzip.open(file_path,'rb') as f:\n",
        "        print(sum(bl.decode().count(\"\\n\") for bl in blocks(f)))\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1m0bd5gYied"
      },
      "source": [
        "As we can see below, the training data is structured as a dictionary of purchases, searches and views associated with a item bought. We cannot see searches or purchases on the example below, but we know they exist because is shown on the [documentation](https://ml-challenge.mercadolibre.com/downloads) of the challenge. Also, we want to know the number of lines in this file because it's important to create DataLoader that will help us loading batches of records on training instead of loading all the dataset at the same time. There are ```413163``` lines in this file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT81O5wWYddE",
        "outputId": "3cae9a63-2100-43a5-ff43-a46cf1edb2ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print_first_lines(RAW_TRAIN_DATASET_PATH)\n",
        "print_num_lines(RAW_TRAIN_DATASET_PATH)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"user_history\": [\n",
            "        {\n",
            "            \"event_info\": 1786148,\n",
            "            \"event_timestamp\": \"2019-10-19T11:25:42.444-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1786148,\n",
            "            \"event_timestamp\": \"2019-10-19T11:25:57.487-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": \"RELOGIO SMARTWATCH\",\n",
            "            \"event_timestamp\": \"2019-10-19T11:26:07.063-0400\",\n",
            "            \"event_type\": \"search\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-19T11:27:26.879-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-19T11:28:36.558-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-19T11:28:40.827-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-19T11:30:42.089-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-19T21:51:29.622-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-19T21:52:09.281-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-19T21:52:41.863-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-19T21:54:16.119-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-19T21:54:40.629-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-19T21:54:57.329-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-19T22:00:04.577-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-20T10:36:47.525-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-20T10:37:23.202-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-20T10:37:47.699-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-20T19:28:14.619-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1615991,\n",
            "            \"event_timestamp\": \"2019-10-20T19:28:41.646-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        }\n",
            "    ],\n",
            "    \"item_bought\": 1748830\n",
            "}\n",
            "{\n",
            "    \"user_history\": [\n",
            "        {\n",
            "            \"event_info\": 643652,\n",
            "            \"event_timestamp\": \"2019-10-06T18:02:53.893-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": \"DESMAMADEIRA ELETRICA\",\n",
            "            \"event_timestamp\": \"2019-10-07T09:45:29.322-0400\",\n",
            "            \"event_type\": \"search\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1156086,\n",
            "            \"event_timestamp\": \"2019-10-07T09:46:07.960-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": \"DESMAMADEIRA ELETRICA\",\n",
            "            \"event_timestamp\": \"2019-10-07T09:46:17.100-0400\",\n",
            "            \"event_type\": \"search\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": \"DESMAMADEIRA ELETRICA\",\n",
            "            \"event_timestamp\": \"2019-10-07T09:46:19.173-0400\",\n",
            "            \"event_type\": \"search\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1943604,\n",
            "            \"event_timestamp\": \"2019-10-07T09:47:53.958-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": \"DESMAMADEIRA ELETRICA\",\n",
            "            \"event_timestamp\": \"2019-10-07T18:53:20.113-0400\",\n",
            "            \"event_type\": \"search\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 206667,\n",
            "            \"event_timestamp\": \"2019-10-07T18:53:26.670-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": \"DESMAMADEIRA ELETRICA\",\n",
            "            \"event_timestamp\": \"2019-10-07T18:54:36.944-0400\",\n",
            "            \"event_type\": \"search\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 1282813,\n",
            "            \"event_timestamp\": \"2019-10-07T18:54:50.998-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 228737,\n",
            "            \"event_timestamp\": \"2019-10-07T18:56:43.678-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 228737,\n",
            "            \"event_timestamp\": \"2019-10-07T19:01:44.718-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 228737,\n",
            "            \"event_timestamp\": \"2019-10-07T19:46:18.382-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        }\n",
            "    ],\n",
            "    \"item_bought\": 228737\n",
            "}\n",
            "{\n",
            "    \"user_history\": [\n",
            "        {\n",
            "            \"event_info\": 248595,\n",
            "            \"event_timestamp\": \"2019-10-01T12:46:03.145-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        },\n",
            "        {\n",
            "            \"event_info\": 248595,\n",
            "            \"event_timestamp\": \"2019-10-01T13:21:50.697-0400\",\n",
            "            \"event_type\": \"view\"\n",
            "        }\n",
            "    ],\n",
            "    \"item_bought\": 1909110\n",
            "}\n",
            "413162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFzY_3XDZNAK"
      },
      "source": [
        "When the event type on the user history is a view or a purchase, then we have an ```event_info``` associated with an ```item_id``` on the file ```item_data.jl.gz``` that has the following information:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfccTW_5Q_Pw",
        "outputId": "70155751-429e-4db9-fff7-478affe991b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print_first_lines(ITEM_DATA_PATH)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"item_id\": 111260,\n",
            "    \"title\": \"Casa Sola En Venta Con Gran Patio Solo Pago De Contado.\",\n",
            "    \"domain_id\": \"MLM-INDIVIDUAL_HOUSES_FOR_SALE\",\n",
            "    \"product_id\": null,\n",
            "    \"price\": \"1150000.00\",\n",
            "    \"category_id\": \"MLM170527\",\n",
            "    \"condition\": \"new\"\n",
            "}\n",
            "{\n",
            "    \"item_id\": 871377,\n",
            "    \"title\": \"Resident Evil Origins Collection Nintendo Switch (en D3gamer\",\n",
            "    \"domain_id\": \"MLM-VIDEO_GAMES\",\n",
            "    \"product_id\": \"15270800\",\n",
            "    \"price\": \"1392.83\",\n",
            "    \"category_id\": \"MLM151595\",\n",
            "    \"condition\": \"new\"\n",
            "}\n",
            "{\n",
            "    \"item_id\": 490232,\n",
            "    \"title\": \"Falda De Imitaci\\u00f3n Piel Negra\",\n",
            "    \"domain_id\": \"MLM-SKIRTS\",\n",
            "    \"product_id\": null,\n",
            "    \"price\": \"350.00\",\n",
            "    \"category_id\": \"MLM7697\",\n",
            "    \"condition\": \"new\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-niKRTMqazOu"
      },
      "source": [
        "## Preparing the dataset\n",
        "\n",
        "We structure the item data in a dictionary to access that info faster to augment the Dataset Loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVqakmtuxnbe"
      },
      "source": [
        "import gzip\n",
        "import ujson\n",
        "\n",
        "item_details = {}\n",
        "with gzip.open(ITEM_DATA_PATH,'rt') as f:\n",
        "    for index, line in enumerate(f):\n",
        "        data = ujson.loads(line)\n",
        "        item_id = data['item_id']\n",
        "        item_details[item_id] = data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTXUJ_cU9Oun"
      },
      "source": [
        "The first thing I need to do is to split the train dataset between validation and training set. This allows me to fine tune the model without submitting it to the platform each time. The submissions per day are limited to 3 so I prefer to validate my model before submission. In this process, I'm going to delete the item of categories that only have one sample to reduce the complexity of the problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4TAuvnK9aWd",
        "outputId": "36a6e579-1136-4550-f070-e26596c29e25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gzip\n",
        "import ujson\n",
        "import random\n",
        "import math\n",
        "\n",
        "\n",
        "def write_dataset(lines, file_path):\n",
        "  with gzip.open(file_path, 'wt') as f:\n",
        "      for index, line in enumerate(lines):\n",
        "          new_line = line.strip(' \\n\\r\\t')\n",
        "          if index == 0:\n",
        "              f.write(new_line)\n",
        "          else:\n",
        "              f.write(f'\\n{new_line}')\n",
        "\n",
        "# We create a dictionary associating domains with their frequency on the\n",
        "# training set\n",
        "classes = []\n",
        "with gzip.open(RAW_TRAIN_DATASET_PATH,'rt') as f:\n",
        "    classes = [item_details[ujson.loads(x)['item_bought']]['domain_id'] for x in f.readlines()]\n",
        "domain_counter = Counter(classes)\n",
        "\n",
        "lines = []\n",
        "classes = []\n",
        "with gzip.open(RAW_TRAIN_DATASET_PATH,'rt') as f:\n",
        "    for line in f:\n",
        "        data = ujson.loads(line)\n",
        "        item_bought = data['item_bought']\n",
        "        domain_id = item_details[item_bought]['domain_id']\n",
        "        if domain_counter[domain_id] > 1:\n",
        "            lines.append(line)\n",
        "            classes.append(domain_id)\n",
        "\n",
        "# Order randomlly the data to improve the SGD on the model\n",
        "train_lines, val_lines = train_test_split(\n",
        "    lines, test_size=0.2, stratify=classes, shuffle=True, random_state=42)\n",
        "\n",
        "write_dataset(train_lines, TRAIN_DATASET_PATH)\n",
        "write_dataset(val_lines, VALIDATION_DATASET_PATH)\n",
        "\n",
        "NUM_TRAINING_SAMPLES = len(train_lines)\n",
        "NUM_VALIDATION_SAMPLES = len(val_lines)\n",
        "print(f'Number of lines for training: {NUM_TRAINING_SAMPLES}')\n",
        "print(f'Number of lines for validation: {NUM_VALIDATION_SAMPLES}')\n",
        "\n",
        "train_lines = None\n",
        "val_lines = None"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of lines for training: 371507\n",
            "Number of lines for validation: 41279\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU6iciEXLGR-"
      },
      "source": [
        "### Oversampling\n",
        "\n",
        "I'm going to increase the number of samples per domain for underrepresented domains. The threshold I'll use is **55** samples because it is the median of the samples per domain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUZLk2pGKjn6",
        "outputId": "ff1c35f6-cad9-4626-d3b9-432e6b1a42b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from collections import Counter\n",
        "import gzip\n",
        "import ujson\n",
        "import math\n",
        "import random\n",
        "\n",
        "# We create a dictionary associating domains with their frequency on the\n",
        "# training set\n",
        "classes = []\n",
        "with gzip.open(TRAIN_DATASET_PATH,'rt') as f:\n",
        "    classes = [item_details[ujson.loads(x)['item_bought']]['domain_id'] for x in f.readlines()]\n",
        "domain_counter = Counter(classes)\n",
        "classes = []\n",
        "\n",
        "# Oversampling\n",
        "threshold = 235\n",
        "lines = []\n",
        "with gzip.open(TRAIN_DATASET_PATH,'rt') as f:\n",
        "    for line in f:\n",
        "        record = ujson.loads(line)\n",
        "        item_id = record['item_bought']\n",
        "        item = item_details[item_id]\n",
        "        domain_id = item['domain_id']\n",
        "\n",
        "        if domain_counter[domain_id] < threshold:\n",
        "            # Oversampling\n",
        "            num_samples = math.floor(threshold * 1.0 / domain_counter[domain_id])\n",
        "            sample_probability = (threshold * 1.0 / domain_counter[domain_id]) - num_samples\n",
        "            random_seed = random.uniform(0, 1)\n",
        "\n",
        "            if num_samples > 0:\n",
        "                for index in range(num_samples):\n",
        "                    lines.append(line)\n",
        "\n",
        "            if random_seed <= sample_probability:\n",
        "                lines.append(line)\n",
        "        else:\n",
        "            lines.append(line)\n",
        "\n",
        "# Shuffle the dataset to avoid having duplicated data on continuous lines\n",
        "random.shuffle(lines)\n",
        "\n",
        "with gzip.open(OVERSAMPLED_TRAIN_DATASET_PATH, 'wt') as f:\n",
        "    for index, line in enumerate(lines):\n",
        "        new_line = line.strip(' \\n\\r\\t')\n",
        "        if index == 0:\n",
        "            f.write(new_line)\n",
        "        else:\n",
        "            f.write(f'\\n{new_line}')\n",
        "\n",
        "NUM_OVERSAMPLED_TRAINING_SAMPLES = len(lines)\n",
        "print(f'Number of lines for training: {NUM_OVERSAMPLED_TRAINING_SAMPLES}')\n",
        "lines = None\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of lines for training: 879286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwRfaHQbQOw0"
      },
      "source": [
        "NUM_TRAINING_SAMPLES = 371507\n",
        "NUM_VALIDATION_SAMPLES = 41279\n",
        "NUM_OVERSAMPLED_TRAINING_SAMPLES = 879286\n",
        "\n",
        "TRAIN_DATASET_PATH = OVERSAMPLED_TRAIN_DATASET_PATH\n",
        "NUM_TRAINING_SAMPLES = NUM_OVERSAMPLED_TRAINING_SAMPLES"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf_9WGjiAkXc"
      },
      "source": [
        "Based on the idea that possibly not all the items in the set of item_details were bought in the training dataset. I will predict only the items and domains that appear in the training dataset. At the same time, I'm going to get some stats about the distribution of domains in the training set to know how oversample the minority of domains.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u954dcecBXpY"
      },
      "source": [
        "import gzip\n",
        "import ujson\n",
        "\n",
        "domain_map = {}\n",
        "categories_map = {}\n",
        "reduced_feature_map = {}\n",
        "domain_freq = {}\n",
        "top_items_per_domain = {}\n",
        "with gzip.open(TRAIN_DATASET_PATH,'rt') as f:\n",
        "  for line in f:\n",
        "    record = ujson.loads(line)\n",
        "    item_id = record['item_bought']\n",
        "    item = item_details[item_id]\n",
        "    domain_id = item['domain_id']\n",
        "    category_id = item['category_id']\n",
        "\n",
        "    # Find the number of times a domain appears on the training set\n",
        "    domain_freq[domain_id] = domain_freq.get(domain_id, 0) + 1\n",
        "\n",
        "    # Map domains to integers\n",
        "    if domain_id not in domain_map:\n",
        "      domain_map[domain_id] = len(domain_map)\n",
        "\n",
        "    # Map categories to integers\n",
        "    if category_id not in categories_map:\n",
        "      categories_map[category_id] = len(categories_map)\n",
        "\n",
        "    # Creating the dependencies in the prediction\n",
        "    if domain_id not in reduced_feature_map:\n",
        "      reduced_feature_map[domain_id] = {}\n",
        "    if item_id not in reduced_feature_map[domain_id]:\n",
        "      reduced_feature_map[domain_id][item_id] = len(reduced_feature_map[domain_id])\n",
        "    \n",
        "    # Finding the most bought items per domain\n",
        "    if domain_map[domain_id] not in top_items_per_domain:\n",
        "      top_items_per_domain[domain_map[domain_id]] = {}\n",
        "    top_items_per_domain[domain_map[domain_id]][item_id] = top_items_per_domain[domain_map[domain_id]].get(item_id, 0) + 1\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VvzIXInFGkM",
        "outputId": "cf04cc1a-7d70-4c04-b57d-1c8fa7f51750",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(f'Items size: {len(item_details)}')\n",
        "print(f'Domains size: {len(domain_map)}')\n",
        "print(f'Categories size: {len(categories_map)}')\n",
        "features_sizes = [len(reduced_feature_map[domain]) for domain in reduced_feature_map]\n",
        "print(f'Min Features: {min(features_sizes)}')\n",
        "print(f'Max Features: {max(features_sizes)}')\n",
        "print(f'Average Features: {sum(features_sizes)/len(features_sizes)}')\n",
        "sorted_freqs = {k: v for k, v in sorted(domain_freq.items(), key=lambda item: item[1], reverse=False)}\n",
        "print(list(sorted_freqs.keys())[:5])\n",
        "print(list(sorted_freqs.values())[:25])\n",
        "\n",
        "NUM_ITEMS_PER_DOMAIN = max(features_sizes)\n",
        "NUM_DOMAINS = len(domain_map)\n",
        "NUM_CATEGORIES = len(categories_map)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Items size: 2102277\n",
            "Domains size: 2837\n",
            "Categories size: 3615\n",
            "Min Features: 1\n",
            "Max Features: 2513\n",
            "Average Features: 22.058160028198802\n",
            "['MLB-BEER_DISPENSERS', 'MLM-AEROBICS_AND_FITNESS_EQUIPMENT', 'MLM-FACIAL_SKIN_CARE_PRODUCTS', 'MLB-UNDERWEAR_ORGANIZERS', 'MLM-WATER_DISPENSERS']\n",
            "[222, 222, 224, 224, 224, 224, 224, 224, 225, 225, 225, 225, 225, 225, 225, 226, 226, 226, 226, 226, 226, 227, 227, 227, 227]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zz1TvQ0VGtl"
      },
      "source": [
        "Find the percentile 75% for domains to know how many samples need to be oversampled for domains with less than that threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXTSFigoV1Md",
        "outputId": "a9c80eb9-d764-4ab0-ce3d-6fab9b323f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "freq_values = list(domain_freq.values())\n",
        "freq_values.sort()\n",
        "percentile = freq_values[math.ceil(len(freq_values)*75/100)]\n",
        "print(f'P_75: {percentile}')\n",
        "\n",
        "plt.hist(freq_values, bins=100, range=(0, 500))\n",
        "plt.title(\"Histogram\")\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "P_75: 55\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASuElEQVR4nO3df7Ddd13n8efLxvKr2vTHtVOT4C0SZaqzQr1T0sVRpIqloOkflWkXbWCyk5m1IgI7kLqroI5aHIdaxt2OWdq1KIOtFW22sIsxLeOgtHIDtT9hGyGliWkTStNSK0rs2z/OJ/Vwm6S599ycm5zP8zFz5ny/n+/nnO/nc3PyOp/z+X7P96SqkCT14VuWugGSpPEx9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoa+IluTfJq5e6HdKxwNDXcS/JjiQ/NqfszUk+BVBV31dVn3yO55hOUkmWHcWmSkvO0JfGwDcTHSsMfU284U8CSc5NMpvkiSSPJHl/q/ZX7X5fkieTnJfkW5L89yQPJtmT5ENJTh563svatkeT/PKc/bw3yU1J/ijJE8Cb274/nWRfkt1Jfi/JiUPPV0l+LskDSb6W5NeTfHeSv2ntvXG4vrQQhr56czVwdVV9O/DdwI2t/Ifb/fKqOqmqPg28ud1+FHgJcBLwewBJzgb+J/Am4EzgZGDFnH2tBW4ClgMfBv4VeDtwOnAecD7wc3Me8xPADwJrgHcBm4CfAVYB3w9cOkLfJUNfE+PP2wh6X5J9DAL5YL4BvDTJ6VX1ZFXdfpjnfBPw/qr6YlU9CVwBXNKmai4G/k9Vfaqq/gX4FWDuhaw+XVV/XlVPV9U/VdW2qrq9qvZX1Q7g94EfmfOY366qJ6rqXuAe4C/a/h8H/i/wiiP/k0jPZuhrUlxUVcsP3Hj2CPqA9cD3AJ9P8pkkbzjMc34n8ODQ+oPAMuCMtu2hAxuq6ing0TmPf2h4Jcn3JLklycNtyuc3GYz6hz0ytPxPB1k/6TDtlZ6Toa+uVNUDVXUp8B3A+4CbkryIZ4/SAf4B+K6h9RcD+xkE8W5g5YENSV4AnDZ3d3PWrwE+D6xu00u/BGThvZHmz9BXV5L8TJKpqnoa2NeKnwb2tvuXDFX/CPD2JGclOYnByPyGqtrPYK7+J5P8x3Zw9b08d4B/G/AE8GSSlwH/ZbH6JR0pQ1+9uQC4N8mTDA7qXtLm258CfgP463ZcYA1wHfCHDM7s+RLwdeCtAG3O/a3AHzMY9T8J7AH++TD7/q/AfwK+Bvwv4IbF7550ePFHVKTRtU8C+xhM3XxpqdsjHYojfWmBkvxkkhe2YwK/A9wN7FjaVkmHZ+hLC7eWwcHefwBWM5gq8qOzjmlO70hSRxzpS1JHjumLQJ1++uk1PT291M2QpOPKtm3bvlJVUwfbdkyH/vT0NLOzs0vdDEk6riR58FDbnN6RpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHXnO0E9yXft90HuGyk5NsqX9lueWJKe08iT5QJLtSe5Kcs7QY9a1+g8kWXd0uiNJOpwjGen/AYPL0Q7bCGytqtXA1rYO8DoG1yBZDWxg8KMRJDkVeA/wSuBc4D0H3igkSePznKFfVX8FfHVO8Vrg+rZ8PXDRUPmHauB2YHmSMxn82POWqvpqVT0GbOHZbySSpKNsod/IPaOqdrflhxn8ZijACr75d0F3trJDlT9Lkg0MPiXw4he/eIHNG5je+LFnlndc+fqRnkuSJsHIB3LbpWQX7VKdVbWpqmaqamZq6qCXjpAkLdBCQ/+RNm1Du9/TyncBq4bqrWxlhyqXJI3RQkN/M3DgDJx1wM1D5Ze1s3jWAI+3aaBPAK9Ncko7gPvaViZJGqPnnNNP8hHg1cDpSXYyOAvnSuDGJOuBB4E3tuofBy4EtgNPAW8BqKqvJvl14DOt3q9V1dyDw5Kko+w5Q7+qLj3EpvMPUreAyw/xPNcB182rdZKkReU3ciWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdGSn0k7w9yb1J7knykSTPT3JWkjuSbE9yQ5ITW93ntfXtbfv0YnRAknTkFhz6SVYAvwDMVNX3AycAlwDvA66qqpcCjwHr20PWA4+18qtaPUnSGI06vbMMeEGSZcALgd3Aa4Cb2vbrgYva8tq2Ttt+fpKMuH9J0jwsOPSrahfwO8CXGYT948A2YF9V7W/VdgIr2vIK4KH22P2t/mkL3b8kaf5Gmd45hcHo/SzgO4EXAReM2qAkG5LMJpndu3fvqE8nSRoyyvTOjwFfqqq9VfUN4KPAq4DlbboHYCWwqy3vAlYBtO0nA4/OfdKq2lRVM1U1MzU1NULzJElzjRL6XwbWJHlhm5s/H7gPuA24uNVZB9zclje3ddr2W6uqRti/JGmeRpnTv4PBAdnPAne359oEvBt4R5LtDObsr20PuRY4rZW/A9g4QrslSQuw7LmrHFpVvQd4z5ziLwLnHqTu14GfHmV/kqTR+I1cSeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyEihn2R5kpuSfD7J/UnOS3Jqki1JHmj3p7S6SfKBJNuT3JXknMXpgiTpSI060r8a+H9V9TLgB4D7gY3A1qpaDWxt6wCvA1a32wbgmhH3LUmapwWHfpKTgR8GrgWoqn+pqn3AWuD6Vu164KK2vBb4UA3cDixPcuaCWy5JmrdRRvpnAXuB/53kc0k+mORFwBlVtbvVeRg4oy2vAB4aevzOVvZNkmxIMptkdu/evSM0T5I01yihvww4B7imql4B/CP/PpUDQFUVUPN50qraVFUzVTUzNTU1QvMkSXONEvo7gZ1VdUdbv4nBm8AjB6Zt2v2etn0XsGro8StbmSRpTBYc+lX1MPBQku9tRecD9wGbgXWtbB1wc1veDFzWzuJZAzw+NA0kSRqDZSM+/q3Ah5OcCHwReAuDN5Ibk6wHHgTe2Op+HLgQ2A481epKksZopNCvqjuBmYNsOv8gdQu4fJT9SZJG4zdyJakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOLFvqBozL9MaPPbO848rXL2FLJGnpONKXpI4Y+pLUEUNfkjpi6EtSR0YO/SQnJPlcklva+llJ7kiyPckNSU5s5c9r69vb9ulR9y1Jmp/FGOm/Dbh/aP19wFVV9VLgMWB9K18PPNbKr2r1JEljNFLoJ1kJvB74YFsP8BrgplbleuCitry2rdO2n9/qS5LGZNSR/u8C7wKebuunAfuqan9b3wmsaMsrgIcA2vbHW/1vkmRDktkks3v37h2xeZKkYQsO/SRvAPZU1bZFbA9VtamqZqpqZmpqajGfWpK6N8o3cl8F/FSSC4HnA98OXA0sT7KsjeZXArta/V3AKmBnkmXAycCjI+xfkjRPCx7pV9UVVbWyqqaBS4Bbq+pNwG3Axa3aOuDmtry5rdO231pVtdD9S5Lm72icp/9u4B1JtjOYs7+2lV8LnNbK3wFsPAr7liQdxqJccK2qPgl8si1/ETj3IHW+Dvz0YuxPkrQwfiNXkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerIsqVuwFKY3vixZ5Z3XPn6JWyJJI2XI31J6oihL0kdMfQlqSOGviR1ZMGhn2RVktuS3Jfk3iRva+WnJtmS5IF2f0orT5IPJNme5K4k5yxWJyRJR2aUkf5+4J1VdTawBrg8ydnARmBrVa0GtrZ1gNcBq9ttA3DNCPuWJC3AgkO/qnZX1Wfb8teA+4EVwFrg+lbteuCitrwW+FAN3A4sT3LmglsuSZq3RZnTTzINvAK4Azijqna3TQ8DZ7TlFcBDQw/b2cokSWMycugnOQn4U+AXq+qJ4W1VVUDN8/k2JJlNMrt3795RmydJGjJS6Cf5VgaB/+Gq+mgrfuTAtE2739PKdwGrhh6+spV9k6raVFUzVTUzNTU1SvMkSXOMcvZOgGuB+6vq/UObNgPr2vI64Oah8svaWTxrgMeHpoEkSWMwyrV3XgX8LHB3kjtb2S8BVwI3JlkPPAi8sW37OHAhsB14CnjLCPuWJC3AgkO/qj4F5BCbzz9I/QIuX+j+JEmj8xu5ktQRQ1+SOmLoS1JHDH1J6oihL0kd6fLnEof504mSeuJIX5I6YuhLUke6n94Z5lSPpEnnSF+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xFM2D8HTNyVNIkf6ktQRQ1+SOmLoS1JHDH1J6ogHco+AB3UlTQpH+pLUEUNfkjpi6EtSR5zTn6dDze8Plw/zGICkY4kjfUnqiCP9ERxqdC9JxypH+pLUEUf6R5lz/ZKOJY70JakjjvSXyJF8y9dvAktabIb+MeBIDgj7BiBpMRj6x6HFPE7gJw6pL2MP/SQXAFcDJwAfrKorx92GSXW4TwxH8kWyo/EGcCSfYnwjkcZnrKGf5ATgfwA/DuwEPpNkc1XdN8529Gi+3ymY75TTYhnlTeJIvi3tG8y/8+/Sp1TV+HaWnAe8t6p+oq1fAVBVv3Ww+jMzMzU7O7vg/fnlKR3PjuTT2bFivm+yR+MNZ74DhoUMMEbp2zgv4ZJkW1XNHHTbmEP/YuCCqvrPbf1ngVdW1c8P1dkAbGir3wt8YYRdng58ZYTHH2966y/Y517Y5/n5rqqaOtiGY+5AblVtAjYtxnMlmT3Uu90k6q2/YJ97YZ8Xz7i/nLULWDW0vrKVSZLGYNyh/xlgdZKzkpwIXAJsHnMbJKlbY53eqar9SX4e+ASDUzavq6p7j+IuF2Wa6DjSW3/BPvfCPi+SsR7IlSQtLS+4JkkdMfQlqSMTGfpJLkjyhSTbk2xc6vYsliTXJdmT5J6hslOTbEnyQLs/pZUnyQfa3+CuJOcsXcsXLsmqJLcluS/JvUne1sontt9Jnp/kb5P8Xevzr7bys5Lc0fp2QzsZgiTPa+vb2/bppWz/QiU5IcnnktzS1ie9vzuS3J3kziSzreyov64nLvSHLvXwOuBs4NIkZy9tqxbNHwAXzCnbCGytqtXA1rYOg/6vbrcNwDVjauNi2w+8s6rOBtYAl7d/z0nu9z8Dr6mqHwBeDlyQZA3wPuCqqnop8BiwvtVfDzzWyq9q9Y5HbwPuH1qf9P4C/GhVvXzofPyj/7quqom6AecBnxhavwK4YqnbtYj9mwbuGVr/AnBmWz4T+EJb/n3g0oPVO55vwM0Mrt3URb+BFwKfBV7J4NuZy1r5M69zBmfDndeWl7V6Weq2z7OfK1vIvQa4Bcgk97e1fQdw+pyyo/66nriRPrACeGhofWcrm1RnVNXutvwwcEZbnri/Q/sY/wrgDia8322q405gD7AF+HtgX1Xtb1WG+/VMn9v2x4HTxtvikf0u8C7g6bZ+GpPdX4AC/iLJtnb5GRjD6/qYuwyDFq6qKslEnoOb5CTgT4FfrKonkjyzbRL7XVX/Crw8yXLgz4CXLXGTjpokbwD2VNW2JK9e6vaM0Q9V1a4k3wFsSfL54Y1H63U9iSP93i718EiSMwHa/Z5WPjF/hyTfyiDwP1xVH23FE99vgKraB9zGYHpjeZIDA7Xhfj3T57b9ZODRMTd1FK8CfirJDuCPGUzxXM3k9heAqtrV7vcweGM/lzG8ricx9Hu71MNmYF1bXsdgzvtA+WXtqP8a4PGhj43HjQyG9NcC91fV+4c2TWy/k0y1ET5JXsDgGMb9DML/4lZtbp8P/C0uBm6tNvF7PKiqK6pqZVVNM/j/emtVvYkJ7S9Akhcl+bYDy8BrgXsYx+t6qQ9mHKUDJBcC/5/BPOh/W+r2LGK/PgLsBr7BYE5vPYO5zK3AA8BfAqe2umFwFtPfA3cDM0vd/gX2+YcYzH3eBdzZbhdOcr+B/wB8rvX5HuBXWvlLgL8FtgN/AjyvlT+/rW9v21+y1H0Yoe+vBm6Z9P62vv1du917IKfG8br2MgyS1JFJnN6RJB2CoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I68m+7SOC3RcmgEAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEUrMjDpyePX"
      },
      "source": [
        "**Dataset Loader:** The dataset loader provides the following features:\n",
        "* The average of the sentence embeddings for item titles for views\n",
        "* The average of the sentence embeddings for searches\n",
        "* A sparse matrix showing the count of domains that appears on views\n",
        "\n",
        "The possible targets for the models are item identifiers, domain numerical identifiers, and categories numerical identifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULJDoAHlcZ5L",
        "outputId": "c77fc0bc-2bb1-49c8-e958-8bd746d07d0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Ideas:\n",
        "# Use the dataset without oversampling\n",
        "# Use the items on views and purchases as possible outcomes\n",
        "# Transform the domain sparse embeddings to a dense one using sentence embeddings\n",
        "#   over the domain_id (it has legible text, so we can use it). Remove the first\n",
        "#   3 letters and the dash, remove the underscores.\n",
        "# Find if we can use the 3 first letters before the dash on domain_id as a separated feature\n",
        "from torch.utils.data import Dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gzip\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import ujson\n",
        "torch.multiprocessing.set_start_method('spawn')# good solution !!!!\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Device: {device}')\n",
        "\n",
        "class MultiTaskDataset(Dataset):\n",
        "    def __init__(self, file_path, config):\n",
        "        self.file_path = file_path\n",
        "        self.config = config\n",
        "        self.sentence_model = SentenceTransformer('average_word_embeddings_glove.6B.300d')\n",
        "        self.sentence_model = self.sentence_model.to(device)\n",
        "        self.f = gzip.open(self.file_path, 'r')\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.config['data_size']\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Getting and preprocessing the inputs\"\"\"\n",
        "        if idx == 0:\n",
        "            # Each time a epoch starts, we close the file, open it again, and\n",
        "            # seek the first line just to be sure it starts at the beginning of\n",
        "            # the file\n",
        "            self.f.close()\n",
        "            self.f = gzip.open(self.file_path, 'r')\n",
        "            self.f.seek(0)\n",
        "        response = {}\n",
        "\n",
        "        # Read the line in the file and transform it in JSON\n",
        "        line = self.f.readline()\n",
        "        try:\n",
        "            raw_data = ujson.loads(line)\n",
        "        except:\n",
        "            # Sometimes the connection to the file closes unexpectedly, so we\n",
        "            # handle that problem here\n",
        "            self.f.close()\n",
        "            self.f = gzip.open(self.file_path, 'r')\n",
        "            self.f.seek(0)\n",
        "            for i in range(idx + 1):\n",
        "                line = self.f.readline()\n",
        "            raw_data = ujson.loads(line)\n",
        "\n",
        "        # Identify the historic of a user and the item bought if exist\n",
        "        # We are going to use the same Data Loader for the test dataset, which\n",
        "        # does not have 'item_bought'\n",
        "        user_history = self.normalize_user_history(raw_data['user_history'])\n",
        "        item_bought = None\n",
        "        if 'item_bought' in raw_data:\n",
        "            item_bought = raw_data['item_bought']\n",
        "\n",
        "        views = []\n",
        "        searches = []\n",
        "        # The embedding size produced by DISTILLBERT is 768. GloVe embeddings\n",
        "        # have 300 dimensions. DISTILLBERT embeddings are more useful than the\n",
        "        # ones produced by GloVe, but they take like 6 times more of computing\n",
        "        # time.\n",
        "        #embedding_size = 768\n",
        "        embedding_size = 300\n",
        "\n",
        "        # By default, we fill the embeddings with zeros\n",
        "        avg_view_embeddings = np.zeros(embedding_size)\n",
        "        avg_search_embeddings = np.zeros(embedding_size)\n",
        "        category_embeddings = np.zeros(NUM_CATEGORIES)\n",
        "        domain_embeddings = np.zeros(NUM_DOMAINS)\n",
        "        price_embedding = np.zeros(5)\n",
        "        condition_embedding = np.zeros(2)\n",
        "        candidates = []\n",
        "\n",
        "        if len(user_history) > 0:\n",
        "\n",
        "            # Assumption #1: The item_bought probably is on the views or at\n",
        "            # least the items are in the same domain or a similar one.\n",
        "\n",
        "            for item in reversed(user_history):\n",
        "                if item['event_type'] != 'search':\n",
        "                    candidates.append(int(item['event_info']))\n",
        "\n",
        "            prices = []\n",
        "            # First position are the new items and the second position is for\n",
        "            # second had items\n",
        "            conditions = [0,0]\n",
        "            for item in user_history:\n",
        "                # Iterate over the items in the user_history and find the text\n",
        "                # in searches or extract the item titles for views.\n",
        "                if item['event_type'] == 'search':\n",
        "                    text = item['event_info']\n",
        "                    searches.append(text)\n",
        "                else:\n",
        "                    # Find the domains that appear on the views\n",
        "                    domain_id = item_details[item['event_info']]['domain_id']\n",
        "                    if domain_id in domain_map:\n",
        "                        domain_embeddings[domain_map[domain_id]] = domain_embeddings[domain_map[domain_id]] + 1\n",
        "                    \n",
        "                    # Find the categories that appear on the views\n",
        "                    category_id = item_details[item['event_info']]['category_id']\n",
        "                    if category_id in categories_map:\n",
        "                        category_embeddings[categories_map[category_id]] = category_embeddings[categories_map[category_id]] + 1\n",
        "\n",
        "                    # Include all the prices of items of views\n",
        "                    price = item_details[item['event_info']]['price']\n",
        "                    if price:\n",
        "                        price = float(price)\n",
        "                        prices.append(price)\n",
        "\n",
        "                    # Include the item condition\n",
        "                    condition = item_details[item['event_info']]['condition']\n",
        "                    if condition == 'new':\n",
        "                        conditions[0] = conditions[0] + 1\n",
        "                    else:\n",
        "                        conditions[1] = conditions[1] + 1\n",
        "\n",
        "                    text = item_details[item['event_info']]['title']\n",
        "                    views.append(text)\n",
        "\n",
        "            # If there are prices, then calculate a simple embedding using\n",
        "            # cumulative metrics like min, max, mean, median, standard deviation\n",
        "            if len(prices) > 0:\n",
        "                prices = np.array(prices)\n",
        "                price_embedding[0] = np.min(prices)\n",
        "                price_embedding[1] = np.max(prices)\n",
        "                price_embedding[2] = np.nanmedian(prices)\n",
        "                price_embedding[3] = np.nanmean(prices)\n",
        "                price_embedding[4] = np.nanstd(prices)\n",
        "            \n",
        "            # If there are conditions for items, then normalize the list\n",
        "            if sum(conditions) > 0:\n",
        "                conditions = np.array(conditions)\n",
        "                condition_embedding[0] = conditions[0] / np.sum(conditions)\n",
        "                condition_embedding[1] = conditions[1] / np.sum(conditions)\n",
        "\n",
        "            # If there are views, then calculate the view embeddings from the\n",
        "            # item title on the views\n",
        "            if len(views) > 0:\n",
        "                view_embeddings = self.sentence_model.encode(views)\n",
        "                avg_view_embeddings = self.embeddings_weighted_average(view_embeddings)\n",
        "\n",
        "            # If there are searches, the calculate the search embeddings from them\n",
        "            if len(searches) > 0:\n",
        "                search_embeddings = self.sentence_model.encode(searches)\n",
        "                avg_search_embeddings = self.embeddings_weighted_average(search_embeddings)\n",
        "\n",
        "        # Assign the values to the features\n",
        "        response['view_embeddings'] = avg_view_embeddings\n",
        "        response['search_embeddings'] = avg_search_embeddings\n",
        "        response['domain_embeddings'] = domain_embeddings\n",
        "        response['category_embeddings'] = category_embeddings\n",
        "        response['price_embedding'] = price_embedding\n",
        "        response['condition_embedding'] = condition_embedding\n",
        "        response['candidates'] = candidates[:10]\n",
        "\n",
        "        # If we are handling the training or validation set, then we also get\n",
        "        # the domain, category and item identifiers\n",
        "        if item_bought:\n",
        "            item_data = item_details[item_bought]\n",
        "            domain_id = item_data['domain_id']\n",
        "            category_id = item_data['category_id']\n",
        "\n",
        "            # We need to verify the values for targets because some items,\n",
        "            # domains, and categories can appear on the trainig set but not\n",
        "            # on the validation set\n",
        "            domain_map_id = NUM_DOMAINS\n",
        "            category_map_id = NUM_CATEGORIES\n",
        "            item_map_id = NUM_ITEMS_PER_DOMAIN\n",
        "            if domain_id in domain_map:\n",
        "                domain_map_id = domain_map[domain_id]\n",
        "                if item_bought in reduced_feature_map.get(domain_id, {}):\n",
        "                    item_map_id = reduced_feature_map[domain_id][item_bought]\n",
        "            if category_id in categories_map:\n",
        "                category_map_id = categories_map[category_id]\n",
        "\n",
        "            # Set the value for the targets\n",
        "            response['item'] = torch.tensor(\n",
        "                item_map_id, dtype=torch.long)\n",
        "            response['domain'] = torch.tensor(\n",
        "                domain_map_id, dtype=torch.long)\n",
        "            response['category'] = torch.tensor(\n",
        "                category_map_id, dtype=torch.long)\n",
        "        return response\n",
        "\n",
        "    def normalize_user_history(sefl, user_history):\n",
        "      # Remove duplicated views and searches that appear almost at the same time\n",
        "      new_user_history = []\n",
        "      last_item_id = ''\n",
        "      for item in user_history:\n",
        "        current_item_id = item['event_info']\n",
        "        if current_item_id != last_item_id:\n",
        "          new_user_history.append(item)\n",
        "        last_item_id = current_item_id\n",
        "      # We are only going to consider the last 15 events\n",
        "      return new_user_history[-20:]\n",
        "\n",
        "    def embeddings_weighted_average(self, embeddings):\n",
        "      # I'm going to use a logarithmic decrease in the importance of each\n",
        "      # view or search according to its recency\n",
        "      embeddings_size = embeddings.shape[0]\n",
        "      avg_embeddings = embeddings\n",
        "      if embeddings_size >= 1:\n",
        "        weights = [1.0 / math.log2(2 + index) for index in range(embeddings_size)]\n",
        "        normal_weigths = [float(w)/sum(weights) for w in weights]\n",
        "        # The last item in the embeddings is the most recent\n",
        "        normal_weigths = np.array(list(reversed(normal_weigths)))\n",
        "        avg_embeddings = embeddings * normal_weigths.reshape((normal_weigths.size, 1))\n",
        "        avg_embeddings = np.sum(avg_embeddings, axis=0)\n",
        "      return avg_embeddings\n",
        "        \n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddlnU955UH_1"
      },
      "source": [
        "Get the data loader for training and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00zSkp9YUJ2U",
        "outputId": "993b30b7-d611-49da-fba0-f2bc23e5e272",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "def my_train_collate(batch):\n",
        "    '''Get features and targets'''\n",
        "    # Features\n",
        "    searches = torch.from_numpy(np.array([item['search_embeddings'] for item in batch]))\n",
        "    views = torch.from_numpy(np.array([item['view_embeddings'] for item in batch]))\n",
        "    domain_views = torch.from_numpy(np.array([item['domain_embeddings'] for item in batch]))\n",
        "    categories = torch.from_numpy(np.array([item['category_embeddings'] for item in batch]))\n",
        "    prices = torch.from_numpy(np.array([item['price_embedding'] for item in batch]))\n",
        "    conditions = torch.from_numpy(np.array([item['condition_embedding'] for item in batch]))\n",
        "    candidates = torch.from_numpy(np.array([item['candidates'] for item in batch]))\n",
        "\n",
        "    # Targets\n",
        "    items = [item['item'] for item in batch]\n",
        "    items = torch.LongTensor(items)\n",
        "    domains = [item['domain'] for item in batch]\n",
        "    domains = torch.LongTensor(domains)\n",
        "    return {\n",
        "        'searches': searches,\n",
        "        'views': views,\n",
        "        'domain_views': domain_views,\n",
        "        'categories': categories,\n",
        "        'prices': prices,\n",
        "        'conditions': conditions,\n",
        "        'candidates': candidates,\n",
        "        'items': items,\n",
        "        'domains': domains\n",
        "    }\n",
        "\n",
        "# Get the Dataset Loader for training\n",
        "training_config = {\n",
        "    'data_size': NUM_TRAINING_SAMPLES,\n",
        "    'batch_size': BATCH_SIZE\n",
        "}\n",
        "ds_train = MultiTaskDataset(TRAIN_DATASET_PATH, training_config)\n",
        "training_loader = DataLoader(\n",
        "    ds_train, batch_size=BATCH_SIZE, num_workers=0, collate_fn=my_train_collate,\n",
        "    pin_memory=True, shuffle=True)\n",
        "\n",
        "# Get the Dataset Loader for validation\n",
        "validation_config = {\n",
        "    'data_size': NUM_VALIDATION_SAMPLES,\n",
        "    'batch_size': BATCH_SIZE\n",
        "}\n",
        "ds_val = MultiTaskDataset(VALIDATION_DATASET_PATH, validation_config)\n",
        "validation_loader = DataLoader(\n",
        "    ds_val, batch_size=BATCH_SIZE, num_workers=0, collate_fn=my_train_collate,\n",
        "    pin_memory=True, shuffle=True)\n",
        "\n",
        "# Notes:\n",
        "# * I found a buggy behavior when num_workers != 0, then I set it that way.\n",
        "# * pin_memory is True to save some memory and speed up the training process.\n",
        "# * shuffle is True to give a little bit of randomness each epoch"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 441M/441M [00:25<00:00, 17.2MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65wVy5Fs_WQr"
      },
      "source": [
        "## Creating the model\n",
        "\n",
        "The model is the ```bert-case-uncased``` version of BERT provided by HugginFaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LymDLqH_yyG"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class MultiTaskModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Creates a MultiTask model for classifications of domains and items based on\n",
        "    the same text\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #num_items = 2102277\n",
        "        num_items_per_domain = NUM_ITEMS_PER_DOMAIN\n",
        "        num_domains = NUM_DOMAINS\n",
        "        num_categories = NUM_CATEGORIES\n",
        "\n",
        "        # The embedding size produced by DISTILLBERT is 768. GloVe embeddings\n",
        "        # have 300 dimensions. DISTILLBERT embeddings are more useful than the\n",
        "        # ones produced by GloVe, but they take like 6 times more of computing\n",
        "        # time.\n",
        "        #embedding_size = 768\n",
        "        embedding_size = 300\n",
        "\n",
        "        # Inputs to hidden layer linear transformation\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.hidden_1 = nn.Linear(embedding_size * 2 + num_domains + 5 + 2, 1024)\n",
        "        self.hidden_1_relu = nn.ReLU()\n",
        "        self.dropout_1 = nn.Dropout(p=0.1)\n",
        "        self.hidden_2 = nn.Linear(1024, 512)\n",
        "        self.hidden_2_relu = nn.ReLU()\n",
        "        self.dropout_2 = nn.Dropout(p=0.1)\n",
        "        self.hidden_3 = nn.Linear(512, 256)\n",
        "        self.hidden_3_relu = nn.ReLU()\n",
        "        self.dropout_3 = nn.Dropout(p=0.1)\n",
        "\n",
        "        # Output layers\n",
        "        self.output_domains = nn.Linear(256, num_domains)\n",
        "        self.domain_relu = nn.ReLU()\n",
        "        self.domain_dropout = nn.Dropout(p=0.1)\n",
        "        #self.output_items = nn.Linear(256, num_items_per_domain)\n",
        "        #self.items_relu = nn.ReLU()\n",
        "        #self.items_dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "        # Define the softmax output\n",
        "        self.log_softmax_domains = nn.LogSoftmax(dim=1)\n",
        "        #self.log_softmax_items = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, views, searches, domain_views, prices, conditions):\n",
        "        # Pass the input tensor through each of our operations\n",
        "        inputs = torch.cat(\n",
        "            (views, searches, domain_views, prices, conditions), dim=1)\n",
        "        x = self.dropout(inputs)\n",
        "        x = self.hidden_1(inputs)\n",
        "        x = self.hidden_1_relu(x)\n",
        "        x = self.dropout_1(x)\n",
        "        x = self.hidden_2(x)\n",
        "        x = self.hidden_2_relu(x)\n",
        "        x = self.dropout_2(x)\n",
        "        x = self.hidden_3(x)\n",
        "        x = self.hidden_3_relu(x)\n",
        "\n",
        "        x_domains = self.output_domains(x)\n",
        "        x_domains = self.domain_relu(x_domains)\n",
        "        x_domains = self.domain_dropout(x_domains)\n",
        "        y_domains = self.log_softmax_domains(x_domains)\n",
        "\n",
        "        #_items = self.output_items(x)\n",
        "        #x_items = self.items_relu(x_items)\n",
        "        #x_items = self.items_dropout(x_items)\n",
        "        #y_items = self.log_softmax_items(x_items)\n",
        "\n",
        "        return y_domains#, y_items"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_tQfQHdI2qg"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_YqJolvGR3p"
      },
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "def get_items_for_domain(domain_id, n=10):\n",
        "    candidates = dict(Counter(top_items_per_domain[domain_id]).most_common(n))\n",
        "    candidates = list(candidates.keys())\n",
        "    while len(candidates) < n:\n",
        "        candidates.append(candidates[0])\n",
        "    return candidates\n",
        "\n",
        "def get_feature_map(reduced_feature_map, domain_map):\n",
        "    features_map = {}\n",
        "    for domain in reduced_feature_map:\n",
        "        for item in reduced_feature_map[domain]:\n",
        "            domain_id = domain_map[domain]\n",
        "            item_id = reduced_feature_map[domain][item]\n",
        "\n",
        "            if domain_id not in features_map:\n",
        "                features_map[domain_id] = {}\n",
        "            if item_id not in features_map[domain_id]:\n",
        "                features_map[domain_id][item_id] = item\n",
        "    return features_map\n",
        "\n",
        "features_map = get_feature_map(reduced_feature_map, domain_map)\n",
        "\n",
        "def get_metric(d_predictions, i_predictions, items, domains):\n",
        "    num_correct = 0\n",
        "    for index, domain in enumerate(d_predictions.tolist()):\n",
        "        candidates = []\n",
        "        true_item = None\n",
        "        true_domain = int(domains[index].data)\n",
        "        if true_domain == domain:\n",
        "            true_item_id = int(items[index].data)\n",
        "            if true_item_id in features_map[domain]:\n",
        "                true_item = features_map[domain][true_item_id]\n",
        "            default_item = list(features_map[domain].values())[0]\n",
        "            for item_candidate in i_predictions[index]:\n",
        "                item_id = features_map[domain].get(item_candidate, None)\n",
        "                if item_id and len(candidates) < 10:\n",
        "                    candidates.append(item_id)\n",
        "            candidates.extend([default_item] * (10 - len(candidates)))\n",
        "\n",
        "        if (len(candidates) > 0) and (true_item in candidates):\n",
        "            index_find = candidates.index(true_item)\n",
        "            numerator = sum([12 / math.log(2 + x) if x == index_find else 1 / math.log(2 + x) for x in range(10)])\n",
        "            denominator = sum([12 / math.log(2 + x) if x == 0 else 1 / math.log(2 + x) for x in range(10)])\n",
        "            num_correct += numerator * 1.0 / denominator\n",
        "        elif len(candidates) > 0:\n",
        "            numerator = sum([1 / math.log(2 + x) for x in range(10)])\n",
        "            denominator = sum([12 / math.log(2 + x) if x == 0 else 1 / math.log(2 + x) for x in range(10)])\n",
        "            num_correct += numerator * 1.0 / denominator\n",
        "        else:\n",
        "            num_correct += 0\n",
        "    return num_correct\n",
        "\n",
        "def get_metric_from_domain(predictions, domains, items):\n",
        "    num_correct = 0\n",
        "    for index, domain in enumerate(predictions.tolist()):\n",
        "        candidates = []\n",
        "        true_item = None\n",
        "        domain_candidates = domain\n",
        "        num_domains = len(domain_candidates)\n",
        "\n",
        "        item_dist = {\n",
        "            1: [10],\n",
        "            2: [5, 5],\n",
        "            3: [4, 3, 3],\n",
        "            4: [3, 3, 2, 2],\n",
        "            5: [2, 2, 2, 2, 2],\n",
        "            6: [2, 2, 2, 2, 1, 1],\n",
        "            7: [2, 2, 2, 1, 1, 1, 1],\n",
        "            8: [2, 2, 1, 1, 1, 1, 1, 1],\n",
        "            9: [2, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "            10: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "        }\n",
        "        true_domain = int(domains[index].data)\n",
        "        domain_pos = None\n",
        "        if true_domain in domain_candidates:\n",
        "            domain_pos = domain_candidates.index(true_domain)\n",
        "            true_item_id = int(items[index].data)\n",
        "            if true_item_id in features_map[true_domain]:\n",
        "                true_item = features_map[true_domain][true_item_id]\n",
        "            num_items = item_dist[num_domains][domain_pos]\n",
        "            candidates = get_items_for_domain(domain_candidates[domain_pos], num_items)\n",
        "\n",
        "        if domain_pos is not None:\n",
        "            start_value = sum(item_dist[num_domains][:domain_pos])\n",
        "            end_value = start_value + item_dist[num_domains][domain_pos]\n",
        "            range_values = range(start_value, end_value)\n",
        "        if (len(candidates) > 0) and (true_item in candidates):\n",
        "            index_find = candidates.index(true_item)\n",
        "            numerator = sum([12 / math.log(2 + x) if x == index_find else 1 / math.log(2 + x) for x in range_values])\n",
        "            denominator = sum([12 / math.log(2 + x) if x == 0 else 1 / math.log(2 + x) for x in range_values])\n",
        "            num_correct += numerator * 1.0 / denominator\n",
        "        elif len(candidates) > 0:\n",
        "            numerator = sum([1 / math.log(2 + x) for x in range_values])\n",
        "            denominator = sum([12 / math.log(2 + x) if x == 0 else 1 / math.log(2 + x) for x in range_values])\n",
        "            num_correct += numerator * 1.0 / denominator\n",
        "        else:\n",
        "            num_correct += 0\n",
        "    return num_correct\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ctQRdTdB9za",
        "outputId": "87a09f4e-07a8-4795-b1ba-b5db79808b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from fastprogress import master_bar, progress_bar\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define model and let it use the GPU\n",
        "#MODEL_PATH = f'{root_path}model-11.pth'\n",
        "model = MultiTaskModel()\n",
        "#model.load_state_dict(torch.load(MODEL_PATH))\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the loss for multi-task classification and let it use the GPU\n",
        "criterion = [nn.NLLLoss(), nn.NLLLoss()]\n",
        "criterion[0] = criterion[0].to(device)\n",
        "criterion[1] = criterion[1].to(device)\n",
        "\n",
        "# Optimizers require the parameters to optimize and a learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "# After the second epoch, the loss does not vary much and I just need to know\n",
        "# what combination of features + model is the best, so I prefer to user the\n",
        "# minimum resources available\n",
        "epochs = 4\n",
        "mb = master_bar(range(epochs))\n",
        "\n",
        "for epoch in mb:\n",
        "    print(f'Epoch: {epoch}')\n",
        "    print('-' * 10)\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Training phase\n",
        "    # ----------------\n",
        "    #model.train()\n",
        "    for record in progress_bar(training_loader, parent=mb):\n",
        "        # Transform features and target to use the GPU\n",
        "        views = record['views'].to(device)\n",
        "        searches = record['searches'].to(device)\n",
        "        domain_views = record['domain_views'].to(device)\n",
        "        #categories = record['categories'].to(device)\n",
        "        prices = record['prices'].to(device)\n",
        "        conditions = record['conditions'].to(device)\n",
        "\n",
        "        items = record['items'].to(device)\n",
        "        domains = record['domains'].to(device)\n",
        "\n",
        "        # Reset the optimizer: Don't reuse info about the last batches\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output_domains = model(\n",
        "            views.float(), searches.float(), domain_views.float(),\n",
        "            prices.float(), conditions.float())\n",
        "        #loss_items = criterion[0](output_items, items)\n",
        "        loss_domains = criterion[0](output_domains, domains)\n",
        "        #loss = loss_items + loss_domains\n",
        "        loss = loss_domains\n",
        "\n",
        "        # backward + optimize only if the model is in training phase\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(f\"Training loss: {running_loss/len(training_loader)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "----------\n",
            "Training loss: 13.660987447061219\n",
            "Epoch: 1\n",
            "----------\n",
            "Training loss: 7.789095156335067\n",
            "Epoch: 2\n",
            "----------\n",
            "Training loss: 7.718504567680748\n",
            "Epoch: 3\n",
            "----------\n",
            "Training loss: 7.968944300453042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diw3Cm4WZKUC",
        "outputId": "f6f3ca5f-294f-497a-9816-c472563fec64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# Validation phase\n",
        "# ----------------\n",
        "model.eval()\n",
        "num_correct_domains = 0\n",
        "num_samples = 0\n",
        "num_correct_items = 0\n",
        "correct_domains = {\n",
        "    1: 0,\n",
        "    2: 0,\n",
        "    3: 0,\n",
        "    4: 0,\n",
        "    5: 0,\n",
        "    6: 0,\n",
        "    7: 0,\n",
        "    8: 0,\n",
        "    9: 0,\n",
        "    10: 0\n",
        "}\n",
        "with torch.no_grad():\n",
        "    for record in progress_bar(validation_loader):\n",
        "        # Prepare features to use the GPU\n",
        "        views = record['views'].to(device)\n",
        "        searches = record['searches'].to(device)\n",
        "        domain_views = record['domain_views'].to(device)\n",
        "        #categories = record['categories'].to(device)\n",
        "        prices = record['prices'].to(device)\n",
        "        conditions = record['conditions'].to(device)\n",
        "\n",
        "        domains = record['domains'].to(device)\n",
        "        items = record['items'].to(device)\n",
        "\n",
        "        # Get the predictions\n",
        "        p_domains = model(\n",
        "            views.float(), searches.float(), domain_views.float(),\n",
        "            prices.float(), conditions.float())\n",
        "        _, d_predictions = p_domains.max(1)\n",
        "        _, d_predictions1 = torch.topk(p_domains, 1)\n",
        "        _, d_predictions2 = torch.topk(p_domains, 2)\n",
        "        _, d_predictions3 = torch.topk(p_domains, 3)\n",
        "        _, d_predictions4 = torch.topk(p_domains, 4)\n",
        "        _, d_predictions5 = torch.topk(p_domains, 5)\n",
        "        _, d_predictions6 = torch.topk(p_domains, 6)\n",
        "        _, d_predictions7 = torch.topk(p_domains, 7)\n",
        "        _, d_predictions8 = torch.topk(p_domains, 8)\n",
        "        _, d_predictions9 = torch.topk(p_domains, 9)\n",
        "        _, d_predictions10 = torch.topk(p_domains, 10)\n",
        "        #_, i_predictions = torch.topk(p_items, 50)\n",
        "\n",
        "        correct_domains[1] += get_metric_from_domain(d_predictions1, domains, items)\n",
        "        correct_domains[2] += get_metric_from_domain(d_predictions2, domains, items)\n",
        "        correct_domains[3] += get_metric_from_domain(d_predictions3, domains, items)\n",
        "        correct_domains[4] += get_metric_from_domain(d_predictions4, domains, items)\n",
        "        correct_domains[5] += get_metric_from_domain(d_predictions5, domains, items)\n",
        "        correct_domains[6] += get_metric_from_domain(d_predictions6, domains, items)\n",
        "        correct_domains[7] += get_metric_from_domain(d_predictions7, domains, items)\n",
        "        correct_domains[8] += get_metric_from_domain(d_predictions8, domains, items)\n",
        "        correct_domains[9] += get_metric_from_domain(d_predictions9, domains, items)\n",
        "        correct_domains[10] += get_metric_from_domain(d_predictions10, domains, items)\n",
        "        # Get metrics\n",
        "        #num_correct_items += get_metric(d_predictions, i_predictions, items, domains)\n",
        "\n",
        "        num_correct_domains += (d_predictions == domains).sum()\n",
        "        num_samples += d_predictions.size(0)\n",
        "    print(f'Got {num_correct_domains} / {num_samples} with accuracy {float(num_correct_domains)/float(num_samples)*100:.2f}')\n",
        "    #print(f'Got {num_correct_items} / {num_samples} with DCG {float(num_correct_items)/float(num_samples):.2f}')\n",
        "    for num_domains in correct_domains:\n",
        "        print(f'# Domains {num_domains} Got {correct_domains[num_domains]} / {num_samples} with DCG {float(correct_domains[num_domains])/float(num_samples):.2f}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='323' class='' max='323' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [323/323 01:48<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Got 4061 / 41279 with accuracy 9.84\n",
            "# Domains 1 Got 1489.3099804823757 / 41279 with DCG 0.04\n",
            "# Domains 2 Got 1748.937197962486 / 41279 with DCG 0.04\n",
            "# Domains 3 Got 1998.0266370786287 / 41279 with DCG 0.05\n",
            "# Domains 4 Got 2124.0158107681423 / 41279 with DCG 0.05\n",
            "# Domains 5 Got 2212.4148437592858 / 41279 with DCG 0.05\n",
            "# Domains 6 Got 2533.4148437592858 / 41279 with DCG 0.06\n",
            "# Domains 7 Got 2791.414843759286 / 41279 with DCG 0.07\n",
            "# Domains 8 Got 2940.31869711749 / 41279 with DCG 0.07\n",
            "# Domains 9 Got 3061.318697117491 / 41279 with DCG 0.07\n",
            "# Domains 10 Got 2962.1666666666665 / 41279 with DCG 0.07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7sbHHNbaIAT"
      },
      "source": [
        "Let's define where to store the model..\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfx3fG-3ZvcD"
      },
      "source": [
        "import torch\n",
        "\n",
        "MODEL_PATH = f'{root_path}model-11.pth'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmlJ5bRpjiXu"
      },
      "source": [
        "Let's store the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9hwJzivjVCu"
      },
      "source": [
        "torch.save(model.state_dict(), MODEL_PATH)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "635LO4z7ZYBX"
      },
      "source": [
        "## Prediction\n",
        "\n",
        "Before sending the answer to this challenge, we need to see the format for the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am8ROroUdvF4",
        "outputId": "b695d715-2602-42c8-94dd-415e00ee2036",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with open(SAMPLE_SUBMISSION_PATH,'rt') as f:\n",
        "    for index, line in enumerate(f):\n",
        "        if index < 4:\n",
        "            print(line)\n",
        "print(f'Number of lines: {index + 1}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "654238,781750,558980,663439,1397720,1095079,798751,1141944,411021,138117\n",
            "\n",
            "462167,1511283,928291,1907892,66135,54134,1090655,700291,63494,613724\n",
            "\n",
            "2092880,1974491,1687910,371918,1659351,156119,578171,1407298,1378300,500637\n",
            "\n",
            "614011,509284,181629,1544217,267392,409673,755307,1621679,767644,617841\n",
            "\n",
            "Number of lines: 177070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUoAvkq0exYP"
      },
      "source": [
        "If Google Colab closed the connection before testing, we can get the model from Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvJOlnbnk_BA"
      },
      "source": [
        "# Optional\n",
        "model = MultiTaskModel()\n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GdLcHZgkQdh"
      },
      "source": [
        "We define the Data Loader for the testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG47EfVHZ_Hc"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "NUM_TESTING_SAMPLES = 177070\n",
        "\n",
        "def my_test_collate(batch):\n",
        "    \"\"\"Get only the features because for testing we have to produce the targets\"\"\"\n",
        "    searches = torch.from_numpy(np.array([item['search_embeddings'] for item in batch]))\n",
        "    views = torch.from_numpy(np.array([item['view_embeddings'] for item in batch]))\n",
        "    domain_views = torch.from_numpy(np.array([item['domain_embeddings'] for item in batch]))\n",
        "    categories = torch.from_numpy(np.array([item['category_embeddings'] for item in batch]))\n",
        "    prices = torch.from_numpy(np.array([item['price_embedding'] for item in batch]))\n",
        "    conditions = torch.from_numpy(np.array([item['condition_embedding'] for item in batch]))\n",
        "    return {\n",
        "        'searches': searches,\n",
        "        'views': views,\n",
        "        'domain_views': domain_views,\n",
        "        'categories': categories,\n",
        "        'prices': prices,\n",
        "        'conditions': conditions\n",
        "    }\n",
        "\n",
        "test_config = {\n",
        "    'data_size': NUM_TESTING_SAMPLES,\n",
        "    'batch_size': BATCH_SIZE\n",
        "}\n",
        "ds_test = MultiTaskDataset(TEST_DATASET_PATH, test_config)\n",
        "test_loader = DataLoader(\n",
        "    ds_test, batch_size=BATCH_SIZE, num_workers=0, collate_fn=my_test_collate,\n",
        "    pin_memory=True)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwPdM4cslVNH"
      },
      "source": [
        "Get the predictions for domains and fill the item identifiers with the most popular items per domain. If there is only one item in a domain repeat it until get the necessary 10 items per record."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkTawRV4jYbx"
      },
      "source": [
        "def get_predictions_for_domains(predictions):\n",
        "    candidates = []\n",
        "    for index, domains in enumerate(predictions.tolist()):\n",
        "        subset = []\n",
        "        for domain in domains:\n",
        "            subset.extend(get_items_for_domain(domain, 1))\n",
        "        candidates.append(subset)\n",
        "    return candidates"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcJEFFrkbHdk",
        "outputId": "d19b4f7f-f178-4f91-dec0-59a31c2a0df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "def get_predictions(model, data_loader):\n",
        "    \"\"\"Get item identifiers that the user will probably buy\"\"\"\n",
        "    model = model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for record in test_loader:\n",
        "            # Prepare features to use the GPU\n",
        "            views = record['views'].to(device)\n",
        "            searches = record['searches'].to(device)\n",
        "            domain_views = record['domain_views'].to(device)\n",
        "            #categories = record['categories'].to(device)\n",
        "            prices = record['prices'].to(device)\n",
        "            conditions = record['conditions'].to(device)\n",
        "\n",
        "            # Get the predictions\n",
        "            domains = model(\n",
        "                views.float(), searches.float(), domain_views.float(),\n",
        "                prices.float(), conditions.float())\n",
        "            _, preds_domains = torch.topk(domains, 10)\n",
        "            preds = get_predictions_for_domains(preds_domains)\n",
        "            #print(preds)\n",
        "            predictions.extend(preds)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "predictions = get_predictions(model, test_loader)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-f782afb8a625>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-f782afb8a625>\u001b[0m in \u001b[0;36mget_predictions\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;31m# Prepare features to use the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'views'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-a91ac5f2561a>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# item title on the views\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviews\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mview_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0mavg_view_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings_weighted_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, is_pretokenized, device, num_workers)\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput_mask_expanded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0memb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m                     \u001b[0mall_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mrelevant_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW0B-lqOmSEN"
      },
      "source": [
        "Store our submission file with the predictions generated above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRACwkbBszix"
      },
      "source": [
        "SUBMISSION_PATH = f'{root_path}real_submission_11.csv'\n",
        "with open(SUBMISSION_PATH,'wb') as f:\n",
        "    preds_str = '\\n'.join([','.join([f'{y}' for y in x]) for x in predictions])\n",
        "    f.write(preds_str.encode())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5JBBTo6BAvC"
      },
      "source": [
        "## References\n",
        "\n",
        "* [BERT Text Classification Using Pytorch](https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b)\n",
        "* [Multi-Task Learning with Pytorch and FastAI](https://towardsdatascience.com/multi-task-learning-with-pytorch-and-fastai-6d10dc7ce855)\n",
        "* [Configuring Google Colab Like A Pro](https://medium.com/@robertbracco1/configuring-google-colab-like-a-pro-d61c253f7573)\n",
        "* [Tuning a Multi-Task Pytorch Network on Fate Grand Order](https://towardsdatascience.com/tuning-a-multi-task-fate-grand-order-trained-pytorch-network-152cfda2e086)\n",
        "* [An Overview of Multi-Task Learning in Deep Neural Networks](https://ruder.io/multi-task/)"
      ]
    }
  ]
}